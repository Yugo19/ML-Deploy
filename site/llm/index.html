<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>llm service - My Docs</title>
        <link href="../css/bootstrap.min.css" rel="stylesheet">
        <link href="../css/font-awesome.min.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css">
        <link href="../assets/_mkdocstrings.css" rel="stylesheet">
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script>hljs.highlightAll();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="..">My Docs</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="navitem">
                                <a href=".." class="nav-link">Welcom</a>
                            </li>
                            <li class="navitem">
                                <a href="../install.md" class="nav-link">Install and run</a>
                            </li>
                            <li class="navitem">
                                <a href="../system-arch/" class="nav-link">System Arch</a>
                            </li>
                            <li class="navitem">
                                <a href="../cnn/" class="nav-link">cnn service</a>
                            </li>
                            <li class="navitem active">
                                <a href="./" class="nav-link">llm service</a>
                            </li>
                            <li class="navitem">
                                <a href="../celery/" class="nav-link">celery config and tasks</a>
                            </li>
                            <li class="navitem">
                                <a href="../models/" class="nav-link">Pydantic model</a>
                            </li>
                            <li class="navitem">
                                <a href="../contrubition-guiline.md" class="nav-link">contrubition guideline</a>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../cnn/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../celery/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="2"><a href="#services.llm.gpt_3_5_turbo" class="nav-link">gpt_3_5_turbo</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="2"><a href="#services.llm.gpt_3_5_turbo.display_chat_history" class="nav-link">display_chat_history</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="2"><a href="#services.llm.gpt_3_5_turbo.get_assistant_response" class="nav-link">get_assistant_response</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="2"><a href="#services.llm.gpt_3_5_turbo.get_response" class="nav-link">get_response</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="2"><a href="#services.llm.llm_preprocessing" class="nav-link">llm_preprocessing</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="2"><a href="#services.llm.llm_preprocessing.chain_workflow" class="nav-link">chain_workflow</a>
              <ul class="nav flex-column">
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<div class="doc doc-object doc-module">



<a id="services.llm.gpt_3_5_turbo"></a>
  <div class="doc doc-contents first">

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h2 id="services.llm.gpt_3_5_turbo.display_chat_history" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">display_chat_history</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Prints the chat history to the console. Each message is displayed with the sender's role and content.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>messages</code></td>
          <td>
                <code>list of dict</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A list of dictionaries where each dictionary represents a message in the chat history.
                     Each message has a 'role' key indicating who sent the message and a 'content' key with the message text.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>app/services/llm/gpt_3_5_turbo.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">display_chat_history</span><span class="p">(</span><span class="n">messages</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Prints the chat history to the console. Each message is displayed with the sender&#39;s role and content.</span>

<span class="sd">    Args:</span>
<span class="sd">        messages (list of dict): A list of dictionaries where each dictionary represents a message in the chat history.</span>
<span class="sd">                                 Each message has a &#39;role&#39; key indicating who sent the message and a &#39;content&#39; key with the message text.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">message</span><span class="p">[</span><span class="s1">&#39;role&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">capitalize</span><span class="p">()</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">message</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h2 id="services.llm.gpt_3_5_turbo.get_assistant_response" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_assistant_response</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Sends the current chat history to the OpenAI API to generate a response from the assistant.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>messages</code></td>
          <td>
                <code>list of dict</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The current chat history as a list of message dictionaries.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Returns:</span></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
<td><code>str</code></td>          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The assistant's response as a string.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Raises:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code>Exception</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Prints an error message if the API call fails and returns a default error response.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>app/services/llm/gpt_3_5_turbo.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_assistant_response</span><span class="p">(</span><span class="n">messages</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sends the current chat history to the OpenAI API to generate a response from the assistant.</span>

<span class="sd">    Args:</span>
<span class="sd">        messages (list of dict): The current chat history as a list of message dictionaries.</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: The assistant&#39;s response as a string.</span>

<span class="sd">    Raises:</span>
<span class="sd">        Exception: Prints an error message if the API call fails and returns a default error response.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4&quot;</span><span class="p">,</span>  <span class="c1"># The model version to use for generating responses, adjust as needed</span>
            <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="n">m</span><span class="p">[</span><span class="s2">&quot;role&quot;</span><span class="p">],</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">m</span><span class="p">[</span><span class="s2">&quot;content&quot;</span><span class="p">]}</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
        <span class="k">return</span> <span class="n">response</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;An error occurred: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="s2">&quot;Sorry, I can&#39;t process your request right now.&quot;</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h2 id="services.llm.gpt_3_5_turbo.get_response" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_response</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Processes a user's prompt to generate and display the assistant's response using the OpenAI GPT model.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>prompt</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The user's message to which the assistant should respond.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Returns:</span></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
<td><code>str</code></td>          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The assistant's response, which is also added to the chat history and displayed along with the rest of the conversation.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>app/services/llm/gpt_3_5_turbo.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_response</span><span class="p">(</span><span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Processes a user&#39;s prompt to generate and display the assistant&#39;s response using the OpenAI GPT model.</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt (str): The user&#39;s message to which the assistant should respond.</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: The assistant&#39;s response, which is also added to the chat history and displayed along with the rest of the conversation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Add the user&#39;s message to the chat history</span>
    <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">})</span>

    <span class="c1"># Get the assistant&#39;s response and add it to the chat history</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">get_assistant_response</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
    <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">response</span><span class="p">})</span>

    <span class="c1"># Display the updated chat history</span>
    <span class="n">display_chat_history</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">response</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-module">



<a id="services.llm.llm_preprocessing"></a>
  <div class="doc doc-contents first">

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h2 id="services.llm.llm_preprocessing.chain_workflow" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">chain_workflow</span><span class="p">(</span><span class="n">openai_api_key</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Initializes and configures a conversational retrieval chain with document understanding and storage capabilities.</p>
<p>This function configures a complete workflow for a conversational AI system, including PDF document loading, 
document splitting, embedding, and compression-based retrieval from a conversational context using LangChain 
and OpenAI technologies. The setup involves creating a vector database if it doesn't exist, and utilizing it for 
context retrieval in conversation.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>openai_api_key</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>API key for OpenAI services, used for both embedding generation and conversational responses.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Returns:</span></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
<td><code>ConversationalRetrievalChain</code></td>          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A configured LangChain conversational retrieval chain, ready for deploying in a chatbot system.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>app/services/llm/llm_preprocessing.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@st</span><span class="o">.</span><span class="n">cache_resource</span>
<span class="k">def</span> <span class="nf">chain_workflow</span><span class="p">(</span><span class="n">openai_api_key</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes and configures a conversational retrieval chain with document understanding and storage capabilities.</span>

<span class="sd">    This function configures a complete workflow for a conversational AI system, including PDF document loading, </span>
<span class="sd">    document splitting, embedding, and compression-based retrieval from a conversational context using LangChain </span>
<span class="sd">    and OpenAI technologies. The setup involves creating a vector database if it doesn&#39;t exist, and utilizing it for </span>
<span class="sd">    context retrieval in conversation.</span>

<span class="sd">    Args:</span>
<span class="sd">        openai_api_key (str): API key for OpenAI services, used for both embedding generation and conversational responses.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ConversationalRetrievalChain: A configured LangChain conversational retrieval chain, ready for deploying in a chatbot system.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">llm_name</span> <span class="o">=</span> <span class="s2">&quot;gpt-3.5-turbo&quot;</span>  <span class="c1"># LLM model name for chat interactions</span>

    <span class="c1"># Directory to persist vector index</span>
    <span class="n">persist_directory</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;VECTOR_INDEX_PATH&#39;</span><span class="p">,</span> <span class="s1">&#39;vector_index/&#39;</span><span class="p">)</span> 

    <span class="c1"># Load OpenAI embedding model</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">OpenAIEmbeddings</span><span class="p">(</span><span class="n">openai_api_key</span><span class="o">=</span><span class="n">openai_api_key</span><span class="p">)</span>

    <span class="c1"># Check if the vectorstore exists</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;DB_PATH&quot;</span><span class="p">,</span> <span class="s2">&quot;vector_index/chroma.sqlite3&quot;</span><span class="p">)):</span>
        <span class="c1"># Load and split documents if vectorstore does not exist</span>
        <span class="n">file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;DOCUMENTS_PATH&quot;</span><span class="p">,</span> <span class="s2">&quot;documents/DoniBara.pdf&quot;</span><span class="p">)</span>
        <span class="n">loader</span> <span class="o">=</span> <span class="n">PyPDFLoader</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
        <span class="n">documents</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>

        <span class="n">text_splitter</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
        <span class="n">splits</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="o">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>

        <span class="n">vectordb</span> <span class="o">=</span> <span class="n">Chroma</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span>
            <span class="n">documents</span><span class="o">=</span><span class="n">splits</span><span class="p">,</span>
            <span class="n">embedding</span><span class="o">=</span><span class="n">embeddings</span><span class="p">,</span>
            <span class="n">persist_directory</span><span class="o">=</span><span class="n">persist_directory</span>
        <span class="p">)</span>

        <span class="n">vectordb</span><span class="o">.</span><span class="n">persist</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vectorstore created and saved successfully. The &#39;chroma.sqlite3&#39; file has been created.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Load existing vectorstore</span>
        <span class="n">vectordb</span> <span class="o">=</span> <span class="n">Chroma</span><span class="p">(</span><span class="n">persist_directory</span><span class="o">=</span><span class="n">persist_directory</span><span class="p">,</span> <span class="n">embedding_function</span><span class="o">=</span><span class="n">embeddings</span><span class="p">)</span>

    <span class="c1"># Load OpenAI chat model</span>
    <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">openai_api_key</span><span class="o">=</span><span class="n">openai_api_key</span><span class="p">)</span>

    <span class="c1"># Setup document compressor and retrieval system</span>
    <span class="n">compressor</span> <span class="o">=</span> <span class="n">LLMChainExtractor</span><span class="o">.</span><span class="n">from_llm</span><span class="p">(</span><span class="n">llm</span><span class="p">)</span>
    <span class="n">compression_retriever</span> <span class="o">=</span> <span class="n">ContextualCompressionRetriever</span><span class="p">(</span>
        <span class="n">base_compressor</span><span class="o">=</span><span class="n">compressor</span><span class="p">,</span>
        <span class="n">base_retriever</span><span class="o">=</span><span class="n">vectordb</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(</span><span class="n">search_type</span><span class="o">=</span><span class="s2">&quot;mmr&quot;</span><span class="p">,</span> <span class="n">search_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;k&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">})</span>
    <span class="p">)</span>

    <span class="c1"># Setup conversation memory for retaining context</span>
    <span class="n">memory</span> <span class="o">=</span> <span class="n">ConversationBufferWindowMemory</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">memory_key</span><span class="o">=</span><span class="s2">&quot;chat_history&quot;</span><span class="p">)</span>

    <span class="c1"># Initialize the conversational retrieval chain</span>
    <span class="n">qa</span> <span class="o">=</span> <span class="n">ConversationalRetrievalChain</span><span class="o">.</span><span class="n">from_llm</span><span class="p">(</span>
        <span class="n">llm</span><span class="o">=</span><span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="n">llm_name</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">openai_api_key</span><span class="o">=</span><span class="n">openai_api_key</span><span class="p">),</span> 
        <span class="n">chain_type</span><span class="o">=</span><span class="s2">&quot;map_reduce&quot;</span><span class="p">,</span> 
        <span class="n">retriever</span><span class="o">=</span><span class="n">compression_retriever</span><span class="p">,</span> 
        <span class="n">memory</span><span class="o">=</span><span class="n">memory</span><span class="p">,</span>
        <span class="n">get_chat_history</span><span class="o">=</span><span class="k">lambda</span> <span class="n">h</span><span class="p">:</span> <span class="n">h</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">qa</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script src="../js/jquery-3.6.0.min.js"></script>
        <script src="../js/bootstrap.min.js"></script>
        <script>
            var base_url = "..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../js/base.js"></script>
        <script src="../search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
