{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcom"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"api_route/","text":"index () Returns a greeting message indicating the API is operational. Returns: Name Type Description dict A dictionary with a message key indicating the service status. Source code in app/apis/main_router.py 13 14 15 16 17 18 19 20 21 @router . get ( '/' ) def index (): \"\"\" Returns a greeting message indicating the API is operational. Returns: dict: A dictionary with a message key indicating the service status. \"\"\" return { 'message' : 'Map Action classification model' } predict_incident_type ( data ) async Receives an image as input and predicts the type of incident depicted in the image. Additionally, it fetches contextual information based on the prediction and stores the results in a database. Parameters: Name Type Description Default data ImageModel The image data received from the client, which includes the image path, sensitivity of structures in the image, and an incident ID. required Returns: Name Type Description JSONResponse A JSON response containing the prediction, probabilities of each predicted class, contextual information, impact analysis, and proposed solutions. Raises: Type Description HTTPException An error response with status code 500 if the image could not be fetched or if there is an issue during processing. Source code in app/apis/main_router.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 @router . post ( '/image/predict' ) async def predict_incident_type ( data : ImageModel ): \"\"\" Receives an image as input and predicts the type of incident depicted in the image. Additionally, it fetches contextual information based on the prediction and stores the results in a database. Args: data (ImageModel): The image data received from the client, which includes the image path, sensitivity of structures in the image, and an incident ID. Returns: JSONResponse: A JSON response containing the prediction, probabilities of each predicted class, contextual information, impact analysis, and proposed solutions. Raises: HTTPException: An error response with status code 500 if the image could not be fetched or if there is an issue during processing. \"\"\" try : # Extract data from ImageModel image_path = data . image_name # Use the correct field name here sensitive_structures = data . sensitive_structures incident_id = data . incident_id # Fetch image and preprocess it response = requests . get ( base_url + image_path ) if response . status_code != 200 : raise HTTPException ( status_code = 500 , detail = f \"Failed to fetch image from { base_url + image_path } \" ) image = response . content # Perform prediction and contextualization prediction_task = perform_prediction . delay ( image ) prediction , probabilities = await prediction_task . get ( timeout = 120 ) context_task = fetch_contextual_information . delay ( prediction , data . sensitive_structures ) get_context , impact , piste_solution = await context_task . get ( timeout = 120 ) query = \"\"\" INSERT INTO \"Mapapi_prediction\" (incident_id, piste_solution, impact_potentiel, context) VALUES (:incident_id, :piste_solution, :impact_potentiel, :context); \"\"\" values = { \"incident_id\" : data . incident_id , \"piste_solution\" : piste_solution , \"impact_potentiel\" : impact , \"context\" : get_context } result = await database . execute ( query = query , values = values ) # Return JSON response return JSONResponse ( content = { \"prediction\" : prediction , \"probabilities\" : probabilities . tolist (), \"context\" : get_context , \"in_depht\" : impact , \"piste_solution\" : piste_solution }) except Exception as e : # Handle exceptions and return appropriate HTTP response raise HTTPException ( status_code = 500 , detail = str ( e ))","title":"Api route"},{"location":"api_route/#apis.main_router.index","text":"Returns a greeting message indicating the API is operational. Returns: Name Type Description dict A dictionary with a message key indicating the service status. Source code in app/apis/main_router.py 13 14 15 16 17 18 19 20 21 @router . get ( '/' ) def index (): \"\"\" Returns a greeting message indicating the API is operational. Returns: dict: A dictionary with a message key indicating the service status. \"\"\" return { 'message' : 'Map Action classification model' }","title":"index"},{"location":"api_route/#apis.main_router.predict_incident_type","text":"Receives an image as input and predicts the type of incident depicted in the image. Additionally, it fetches contextual information based on the prediction and stores the results in a database. Parameters: Name Type Description Default data ImageModel The image data received from the client, which includes the image path, sensitivity of structures in the image, and an incident ID. required Returns: Name Type Description JSONResponse A JSON response containing the prediction, probabilities of each predicted class, contextual information, impact analysis, and proposed solutions. Raises: Type Description HTTPException An error response with status code 500 if the image could not be fetched or if there is an issue during processing. Source code in app/apis/main_router.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 @router . post ( '/image/predict' ) async def predict_incident_type ( data : ImageModel ): \"\"\" Receives an image as input and predicts the type of incident depicted in the image. Additionally, it fetches contextual information based on the prediction and stores the results in a database. Args: data (ImageModel): The image data received from the client, which includes the image path, sensitivity of structures in the image, and an incident ID. Returns: JSONResponse: A JSON response containing the prediction, probabilities of each predicted class, contextual information, impact analysis, and proposed solutions. Raises: HTTPException: An error response with status code 500 if the image could not be fetched or if there is an issue during processing. \"\"\" try : # Extract data from ImageModel image_path = data . image_name # Use the correct field name here sensitive_structures = data . sensitive_structures incident_id = data . incident_id # Fetch image and preprocess it response = requests . get ( base_url + image_path ) if response . status_code != 200 : raise HTTPException ( status_code = 500 , detail = f \"Failed to fetch image from { base_url + image_path } \" ) image = response . content # Perform prediction and contextualization prediction_task = perform_prediction . delay ( image ) prediction , probabilities = await prediction_task . get ( timeout = 120 ) context_task = fetch_contextual_information . delay ( prediction , data . sensitive_structures ) get_context , impact , piste_solution = await context_task . get ( timeout = 120 ) query = \"\"\" INSERT INTO \"Mapapi_prediction\" (incident_id, piste_solution, impact_potentiel, context) VALUES (:incident_id, :piste_solution, :impact_potentiel, :context); \"\"\" values = { \"incident_id\" : data . incident_id , \"piste_solution\" : piste_solution , \"impact_potentiel\" : impact , \"context\" : get_context } result = await database . execute ( query = query , values = values ) # Return JSON response return JSONResponse ( content = { \"prediction\" : prediction , \"probabilities\" : probabilities . tolist (), \"context\" : get_context , \"in_depht\" : impact , \"piste_solution\" : piste_solution }) except Exception as e : # Handle exceptions and return appropriate HTTP response raise HTTPException ( status_code = 500 , detail = str ( e ))","title":"predict_incident_type"},{"location":"celery/","text":"make_celery () Create and configure a Celery application instance with Redis as the message broker and backend. This function sets up Celery with Redis, specifying the same URI for both the backend and broker. This configuration is necessary for task queueing and result storage for distributed task processing. Returns: Name Type Description Celery A Celery application instance configured to use Redis for queuing tasks and storing results. Source code in app/services/celery/celery_config.py 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def make_celery (): \"\"\" Create and configure a Celery application instance with Redis as the message broker and backend. This function sets up Celery with Redis, specifying the same URI for both the backend and broker. This configuration is necessary for task queueing and result storage for distributed task processing. Returns: Celery: A Celery application instance configured to use Redis for queuing tasks and storing results. \"\"\" # Initialize Celery celery = Celery ( 'worker' , # Name of the worker backend = 'redis://yugo:Maoene@redis:6379/0' , # URI for Redis backend (Results backend) broker = 'redis://yugo:Maoene@redis:6379/0' # URI for Redis broker (Queue) ) return celery fetch_contextual_information ( prediction , sensitive_structures ) A Celery task that fetches contextual information based on a given prediction and the sensitivity of certain structures. This function uses language models to generate text responses that provide context, impact assessment, and potential solutions for the predicted event, specifically tailored to an African context and local community management. Parameters: Name Type Description Default prediction str The predicted incident or object from the image. required sensitive_structures list A list of structures or elements identified as sensitive and relevant to the incident. required Returns: Name Type Description tuple A tuple containing contextual information about the incident, its impact on the sensitive structures, and suggested solutions. Source code in app/services/celery/celery_task.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 @celery_app . task def fetch_contextual_information ( prediction , sensitive_structures ): \"\"\" A Celery task that fetches contextual information based on a given prediction and the sensitivity of certain structures. This function uses language models to generate text responses that provide context, impact assessment, and potential solutions for the predicted event, specifically tailored to an African context and local community management. Args: prediction (str): The predicted incident or object from the image. sensitive_structures (list): A list of structures or elements identified as sensitive and relevant to the incident. Returns: tuple: A tuple containing contextual information about the incident, its impact on the sensitive structures, and suggested solutions. \"\"\" get_context = get_response ( f \"What is a { prediction } in an African zone?\" ) impact = get_response ( f \"What is the impact of { prediction } on { sensitive_structures } \" ) piste_solution = get_response ( \"What are the possible solutions for the previous case? Assuming it's managed by a local community.\" ) return get_context , impact , piste_solution perform_prediction ( image ) A Celery task that performs image prediction using a convolutional neural network. This function processes an image to predict its content and calculate the probabilities of different classifications. Parameters: Name Type Description Default image bytes The image data in bytes format, ready to be processed by the prediction model. required Returns: Name Type Description tuple A tuple containing the predicted classification and a list of probabilities associated with each class. Source code in app/services/celery/celery_task.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @celery_app . task def perform_prediction ( image ): \"\"\" A Celery task that performs image prediction using a convolutional neural network. This function processes an image to predict its content and calculate the probabilities of different classifications. Args: image (bytes): The image data in bytes format, ready to be processed by the prediction model. Returns: tuple: A tuple containing the predicted classification and a list of probabilities associated with each class. \"\"\" prediction , probabilities = predict ( image ) return prediction , probabilities . tolist ()","title":"celery config and tasks"},{"location":"celery/#services.celery.celery_config.make_celery","text":"Create and configure a Celery application instance with Redis as the message broker and backend. This function sets up Celery with Redis, specifying the same URI for both the backend and broker. This configuration is necessary for task queueing and result storage for distributed task processing. Returns: Name Type Description Celery A Celery application instance configured to use Redis for queuing tasks and storing results. Source code in app/services/celery/celery_config.py 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def make_celery (): \"\"\" Create and configure a Celery application instance with Redis as the message broker and backend. This function sets up Celery with Redis, specifying the same URI for both the backend and broker. This configuration is necessary for task queueing and result storage for distributed task processing. Returns: Celery: A Celery application instance configured to use Redis for queuing tasks and storing results. \"\"\" # Initialize Celery celery = Celery ( 'worker' , # Name of the worker backend = 'redis://yugo:Maoene@redis:6379/0' , # URI for Redis backend (Results backend) broker = 'redis://yugo:Maoene@redis:6379/0' # URI for Redis broker (Queue) ) return celery","title":"make_celery"},{"location":"celery/#services.celery.celery_task.fetch_contextual_information","text":"A Celery task that fetches contextual information based on a given prediction and the sensitivity of certain structures. This function uses language models to generate text responses that provide context, impact assessment, and potential solutions for the predicted event, specifically tailored to an African context and local community management. Parameters: Name Type Description Default prediction str The predicted incident or object from the image. required sensitive_structures list A list of structures or elements identified as sensitive and relevant to the incident. required Returns: Name Type Description tuple A tuple containing contextual information about the incident, its impact on the sensitive structures, and suggested solutions. Source code in app/services/celery/celery_task.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 @celery_app . task def fetch_contextual_information ( prediction , sensitive_structures ): \"\"\" A Celery task that fetches contextual information based on a given prediction and the sensitivity of certain structures. This function uses language models to generate text responses that provide context, impact assessment, and potential solutions for the predicted event, specifically tailored to an African context and local community management. Args: prediction (str): The predicted incident or object from the image. sensitive_structures (list): A list of structures or elements identified as sensitive and relevant to the incident. Returns: tuple: A tuple containing contextual information about the incident, its impact on the sensitive structures, and suggested solutions. \"\"\" get_context = get_response ( f \"What is a { prediction } in an African zone?\" ) impact = get_response ( f \"What is the impact of { prediction } on { sensitive_structures } \" ) piste_solution = get_response ( \"What are the possible solutions for the previous case? Assuming it's managed by a local community.\" ) return get_context , impact , piste_solution","title":"fetch_contextual_information"},{"location":"celery/#services.celery.celery_task.perform_prediction","text":"A Celery task that performs image prediction using a convolutional neural network. This function processes an image to predict its content and calculate the probabilities of different classifications. Parameters: Name Type Description Default image bytes The image data in bytes format, ready to be processed by the prediction model. required Returns: Name Type Description tuple A tuple containing the predicted classification and a list of probabilities associated with each class. Source code in app/services/celery/celery_task.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @celery_app . task def perform_prediction ( image ): \"\"\" A Celery task that performs image prediction using a convolutional neural network. This function processes an image to predict its content and calculate the probabilities of different classifications. Args: image (bytes): The image data in bytes format, ready to be processed by the prediction model. Returns: tuple: A tuple containing the predicted classification and a list of probabilities associated with each class. \"\"\" prediction , probabilities = predict ( image ) return prediction , probabilities . tolist ()","title":"perform_prediction"},{"location":"cnn/","text":"preprocess_image ( image ) Preprocesses an image for model prediction by resizing, converting it to a tensor, and potentially normalizing it. This function is specifically tailored for models that expect input images of size 224x224 pixels and images in tensor format. Parameters: Name Type Description Default image bytes The image data in bytes format, as received from an image file or a network. required Returns: Type Description torch.Tensor: A PyTorch tensor representing the processed image, ready to be fed into a model. The tensor is unsqueezed, adding a batch dimension at the beginning. Source code in app/services/cnn/cnn_preprocess.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def preprocess_image ( image ): \"\"\" Preprocesses an image for model prediction by resizing, converting it to a tensor, and potentially normalizing it. This function is specifically tailored for models that expect input images of size 224x224 pixels and images in tensor format. Args: image (bytes): The image data in bytes format, as received from an image file or a network. Returns: torch.Tensor: A PyTorch tensor representing the processed image, ready to be fed into a model. The tensor is unsqueezed, adding a batch dimension at the beginning. \"\"\" image = Image . open ( io . BytesIO ( image )) . convert ( \"RGB\" ) # Open and convert image to RGB image = transform ( image ) . unsqueeze ( 0 ) # Apply the transformation and add a batch dimension return image m_a_model ( num_classes ) Initializes and modifies a VGG16 model with batch normalization to classify a specified number of classes. This function loads a pretrained VGG16 model, freezes its parameters to prevent further training of the feature extractor layers, and then adapts the classifier part of the model to the specified number of classes for new training tasks. Parameters: Name Type Description Default num_classes int The number of target classes for the model to classify. required Returns: Type Description torch.nn.Module: A PyTorch model configured for the specified number of classes with all parameters of the feature extractor layers frozen and the classifier layers tailored to the target task. Source code in app/services/cnn/cnn_model.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def m_a_model ( num_classes : int ): \"\"\" Initializes and modifies a VGG16 model with batch normalization to classify a specified number of classes. This function loads a pretrained VGG16 model, freezes its parameters to prevent further training of the feature extractor layers, and then adapts the classifier part of the model to the specified number of classes for new training tasks. Args: num_classes (int): The number of target classes for the model to classify. Returns: torch.nn.Module: A PyTorch model configured for the specified number of classes with all parameters of the feature extractor layers frozen and the classifier layers tailored to the target task. \"\"\" # Load VGG16 model with batch normalization weights vgg16_bn_weights = VGG16_BN_Weights . DEFAULT model = vgg16_bn ( weights = vgg16_bn_weights ) # Freeze all parameters in the model for params in model . parameters (): params . requires_grad = False # Modify the classifier to adapt to the number of classes num_features = model . classifier [ 6 ] . in_features features = list ( model . classifier . children ())[: - 1 ] features . extend ([ torch . nn . Linear ( num_features , num_classes )]) model . classifier = torch . nn . Sequential ( * features ) return model predict ( image ) Performs image classification using a pre-trained VGG16 model modified for six specific categories. This function processes an input image, applies a pre-trained model, and returns the predicted category and probability distribution over all categories. Parameters: Name Type Description Default image bytes The image data in bytes format. required Returns: Name Type Description tuple A tuple containing the predicted category as a string and the probabilities of all categories as a tensor. Source code in app/services/cnn/cnn.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def predict ( image ): \"\"\" Performs image classification using a pre-trained VGG16 model modified for six specific categories. This function processes an input image, applies a pre-trained model, and returns the predicted category and probability distribution over all categories. Args: image (bytes): The image data in bytes format. Returns: tuple: A tuple containing the predicted category as a string and the probabilities of all categories as a tensor. \"\"\" model . eval () # Set the model to evaluation mode input_data = preprocess_image ( image ) # Preprocess the image with torch . no_grad (): # Disable gradient calculation output = model ( input_data ) # Get the model output probabilities = torch . nn . functional . softmax ( output [ 0 ], dim = 0 ) # Calculate probabilities predicted_class = torch . argmax ( probabilities , dim = 0 ) # Determine the predicted class predict_label_index = predicted_class . item () predict_label = categories [ int ( predict_label_index )] # Get the category name from the index return predict_label , probabilities . tolist () # Return the predicted category and probabilities","title":"cnn service"},{"location":"cnn/#services.cnn.cnn_preprocess.preprocess_image","text":"Preprocesses an image for model prediction by resizing, converting it to a tensor, and potentially normalizing it. This function is specifically tailored for models that expect input images of size 224x224 pixels and images in tensor format. Parameters: Name Type Description Default image bytes The image data in bytes format, as received from an image file or a network. required Returns: Type Description torch.Tensor: A PyTorch tensor representing the processed image, ready to be fed into a model. The tensor is unsqueezed, adding a batch dimension at the beginning. Source code in app/services/cnn/cnn_preprocess.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def preprocess_image ( image ): \"\"\" Preprocesses an image for model prediction by resizing, converting it to a tensor, and potentially normalizing it. This function is specifically tailored for models that expect input images of size 224x224 pixels and images in tensor format. Args: image (bytes): The image data in bytes format, as received from an image file or a network. Returns: torch.Tensor: A PyTorch tensor representing the processed image, ready to be fed into a model. The tensor is unsqueezed, adding a batch dimension at the beginning. \"\"\" image = Image . open ( io . BytesIO ( image )) . convert ( \"RGB\" ) # Open and convert image to RGB image = transform ( image ) . unsqueeze ( 0 ) # Apply the transformation and add a batch dimension return image","title":"preprocess_image"},{"location":"cnn/#services.cnn.cnn_model.m_a_model","text":"Initializes and modifies a VGG16 model with batch normalization to classify a specified number of classes. This function loads a pretrained VGG16 model, freezes its parameters to prevent further training of the feature extractor layers, and then adapts the classifier part of the model to the specified number of classes for new training tasks. Parameters: Name Type Description Default num_classes int The number of target classes for the model to classify. required Returns: Type Description torch.nn.Module: A PyTorch model configured for the specified number of classes with all parameters of the feature extractor layers frozen and the classifier layers tailored to the target task. Source code in app/services/cnn/cnn_model.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def m_a_model ( num_classes : int ): \"\"\" Initializes and modifies a VGG16 model with batch normalization to classify a specified number of classes. This function loads a pretrained VGG16 model, freezes its parameters to prevent further training of the feature extractor layers, and then adapts the classifier part of the model to the specified number of classes for new training tasks. Args: num_classes (int): The number of target classes for the model to classify. Returns: torch.nn.Module: A PyTorch model configured for the specified number of classes with all parameters of the feature extractor layers frozen and the classifier layers tailored to the target task. \"\"\" # Load VGG16 model with batch normalization weights vgg16_bn_weights = VGG16_BN_Weights . DEFAULT model = vgg16_bn ( weights = vgg16_bn_weights ) # Freeze all parameters in the model for params in model . parameters (): params . requires_grad = False # Modify the classifier to adapt to the number of classes num_features = model . classifier [ 6 ] . in_features features = list ( model . classifier . children ())[: - 1 ] features . extend ([ torch . nn . Linear ( num_features , num_classes )]) model . classifier = torch . nn . Sequential ( * features ) return model","title":"m_a_model"},{"location":"cnn/#services.cnn.cnn.predict","text":"Performs image classification using a pre-trained VGG16 model modified for six specific categories. This function processes an input image, applies a pre-trained model, and returns the predicted category and probability distribution over all categories. Parameters: Name Type Description Default image bytes The image data in bytes format. required Returns: Name Type Description tuple A tuple containing the predicted category as a string and the probabilities of all categories as a tensor. Source code in app/services/cnn/cnn.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def predict ( image ): \"\"\" Performs image classification using a pre-trained VGG16 model modified for six specific categories. This function processes an input image, applies a pre-trained model, and returns the predicted category and probability distribution over all categories. Args: image (bytes): The image data in bytes format. Returns: tuple: A tuple containing the predicted category as a string and the probabilities of all categories as a tensor. \"\"\" model . eval () # Set the model to evaluation mode input_data = preprocess_image ( image ) # Preprocess the image with torch . no_grad (): # Disable gradient calculation output = model ( input_data ) # Get the model output probabilities = torch . nn . functional . softmax ( output [ 0 ], dim = 0 ) # Calculate probabilities predicted_class = torch . argmax ( probabilities , dim = 0 ) # Determine the predicted class predict_label_index = predicted_class . item () predict_label = categories [ int ( predict_label_index )] # Get the category name from the index return predict_label , probabilities . tolist () # Return the predicted category and probabilities","title":"predict"},{"location":"contribution-guideline/","text":"Map Action Contribution Guidelines Welcome to Map Action! We're excited about your interest in contributing to our open-source project. These guidelines will help you understand how to effectively contribute to our codebase. About Map Action Map Action is a Bamako-based trailblazer in using mapping technology to tackle environmental challenges and solve urban issues. Our innovative approach began by addressing Water, Sanitation, and Hygiene (WASH) problems and has grown to encompass various sectors through collaboration with civil society, governments, NGOs, and private entities. Our sustainable business model includes paid subscriptions for organizations that rely on our data to gain actionable insights across diverse domains. This model allows us to continuously enhance our offerings. Contributing We welcome contributions of all shapes and sizes! Here are the different ways you can get involved: Report bugs and request features: Identify any issues you encounter while using our tools. You can report them directly on GitHub Issues https://docs.github.com/en/issues/tracking-your-work-with-issues/about-issues. Submit code changes: If you have improvements or new features in mind, you can contribute code by creating pull requests (PRs). Write documentation or tutorials: Enhance our documentation to make it easier for others to understand and use our tools. Help with code reviews and testing: Lend your expertise by reviewing pull requests submitted by others and assisting with testing efforts. Finding Issues to Work On Browse our issue tracker on GitHub Issues https://docs.github.com/en/issues/tracking-your-work-with-issues/about-issues to find existing issues. Look for issues labeled \"help wanted\" or those categorized as bugs or enhancements that you're interested in tackling. Making Changes Fork the Repository: Visit the Map Action project on GitHub: https://github.com/223MapAction Click on the \"Fork\" button to create your own copy of the repository. Create a Branch: Clone your forked repository to your local machine. Create a new branch for your specific changes. Use a descriptive branch name that reflects your contribution (e.g., \"fix-map-loading-bug\"). Code and Commit: Make your changes to the codebase. Write clear and concise commit messages that describe your modifications. Testing: Ensure your changes don't introduce any regressions. We use pytest and flake8 for testing. Make sure your code passes all tests before submitting a pull request. Submitting Pull Requests Push to Your Branch: Once you're satisfied with your changes, push your local branch to your forked repository on GitHub. Open a Pull Request: Navigate to your forked repository on GitHub and go to the \"Pull Requests\" tab. Click on \"New pull request\" and select the branch containing your changes. Create a pull request with a clear and descriptive title and explanation of your modifications. Mention any issues your pull request addresses. Code Review: Our internal code review process involves two Map Action developers. They'll review your pull request and provide feedback or suggestions for improvement. Merge: Once your pull request is approved and any necessary changes are made, it will be merged into the main codebase. Additional Notes License: All our repositories use the GPL-3.0 license. Ensure your contributions comply with the license terms. Code of Conduct: We value a respectful and inclusive environment. Please familiarize yourself with our [Code of Conduct](CODE_OF_CONDUCT.md) before contributing. Appreciation: We appreciate all contributions, regardless of their scope. Thank you for helping us improve Map Action! We look forward to your contributions! This tailored guide incorporates the information you provided, making it specific to the Map Action project and its contribution workflow.","title":"Contribution guideline"},{"location":"install-run/","text":"","title":"Install run"},{"location":"llm/","text":"display_chat_history ( messages ) Prints the chat history to the console. Each message is displayed with the sender's role and content. Parameters: Name Type Description Default messages list of dict A list of dictionaries where each dictionary represents a message in the chat history. Each message has a 'role' key indicating who sent the message and a 'content' key with the message text. required Source code in app/services/llm/gpt_3_5_turbo.py 12 13 14 15 16 17 18 19 20 21 def display_chat_history ( messages ): \"\"\" Prints the chat history to the console. Each message is displayed with the sender's role and content. Args: messages (list of dict): A list of dictionaries where each dictionary represents a message in the chat history. Each message has a 'role' key indicating who sent the message and a 'content' key with the message text. \"\"\" for message in messages : print ( f \" { message [ 'role' ] . capitalize () } : { message [ 'content' ] } \" ) get_assistant_response ( messages ) Sends the current chat history to the OpenAI API to generate a response from the assistant. Parameters: Name Type Description Default messages list of dict The current chat history as a list of message dictionaries. required Returns: Name Type Description str The assistant's response as a string. Raises: Type Description Exception Prints an error message if the API call fails and returns a default error response. Source code in app/services/llm/gpt_3_5_turbo.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def get_assistant_response ( messages ): \"\"\" Sends the current chat history to the OpenAI API to generate a response from the assistant. Args: messages (list of dict): The current chat history as a list of message dictionaries. Returns: str: The assistant's response as a string. Raises: Exception: Prints an error message if the API call fails and returns a default error response. \"\"\" try : r = client . chat . completions . create ( model = \"gpt-4\" , # The model version to use for generating responses, adjust as needed messages = [{ \"role\" : m [ \"role\" ], \"content\" : m [ \"content\" ]} for m in messages ], ) response = r . choices [ 0 ] . message . content return response except Exception as e : print ( f \"An error occurred: { e } \" ) return \"Sorry, I can't process your request right now.\" get_response ( prompt ) Processes a user's prompt to generate and display the assistant's response using the OpenAI GPT model. Parameters: Name Type Description Default prompt str The user's message to which the assistant should respond. required Returns: Name Type Description str The assistant's response, which is also added to the chat history and displayed along with the rest of the conversation. Source code in app/services/llm/gpt_3_5_turbo.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def get_response ( prompt : str ): \"\"\" Processes a user's prompt to generate and display the assistant's response using the OpenAI GPT model. Args: prompt (str): The user's message to which the assistant should respond. Returns: str: The assistant's response, which is also added to the chat history and displayed along with the rest of the conversation. \"\"\" # Add the user's message to the chat history messages . append ({ \"role\" : \"user\" , \"content\" : prompt }) # Get the assistant's response and add it to the chat history response = get_assistant_response ( messages ) messages . append ({ \"role\" : \"assistant\" , \"content\" : response }) # Display the updated chat history display_chat_history ( messages ) return response chain_workflow ( openai_api_key ) Initializes and configures a conversational retrieval chain with document understanding and storage capabilities. This function configures a complete workflow for a conversational AI system, including PDF document loading, document splitting, embedding, and compression-based retrieval from a conversational context using LangChain and OpenAI technologies. The setup involves creating a vector database if it doesn't exist, and utilizing it for context retrieval in conversation. Parameters: Name Type Description Default openai_api_key str API key for OpenAI services, used for both embedding generation and conversational responses. required Returns: Name Type Description ConversationalRetrievalChain A configured LangChain conversational retrieval chain, ready for deploying in a chatbot system. Source code in app/services/llm/llm_preprocessing.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 @st . cache_resource def chain_workflow ( openai_api_key ): \"\"\" Initializes and configures a conversational retrieval chain with document understanding and storage capabilities. This function configures a complete workflow for a conversational AI system, including PDF document loading, document splitting, embedding, and compression-based retrieval from a conversational context using LangChain and OpenAI technologies. The setup involves creating a vector database if it doesn't exist, and utilizing it for context retrieval in conversation. Args: openai_api_key (str): API key for OpenAI services, used for both embedding generation and conversational responses. Returns: ConversationalRetrievalChain: A configured LangChain conversational retrieval chain, ready for deploying in a chatbot system. \"\"\" llm_name = \"gpt-3.5-turbo\" # LLM model name for chat interactions # Directory to persist vector index persist_directory = os . environ . get ( 'VECTOR_INDEX_PATH' , 'vector_index/' ) # Load OpenAI embedding model embeddings = OpenAIEmbeddings ( openai_api_key = openai_api_key ) # Check if the vectorstore exists if not os . path . exists ( os . environ . get ( \"DB_PATH\" , \"vector_index/chroma.sqlite3\" )): # Load and split documents if vectorstore does not exist file = os . environ . get ( \"DOCUMENTS_PATH\" , \"documents/DoniBara.pdf\" ) loader = PyPDFLoader ( file ) documents = loader . load () text_splitter = RecursiveCharacterTextSplitter ( chunk_size = 1000 , chunk_overlap = 150 ) splits = text_splitter . split_documents ( documents ) vectordb = Chroma . from_documents ( documents = splits , embedding = embeddings , persist_directory = persist_directory ) vectordb . persist () print ( \"Vectorstore created and saved successfully. The 'chroma.sqlite3' file has been created.\" ) else : # Load existing vectorstore vectordb = Chroma ( persist_directory = persist_directory , embedding_function = embeddings ) # Load OpenAI chat model llm = ChatOpenAI ( temperature = 0.2 , openai_api_key = openai_api_key ) # Setup document compressor and retrieval system compressor = LLMChainExtractor . from_llm ( llm ) compression_retriever = ContextualCompressionRetriever ( base_compressor = compressor , base_retriever = vectordb . as_retriever ( search_type = \"mmr\" , search_kwargs = { \"k\" : 3 }) ) # Setup conversation memory for retaining context memory = ConversationBufferWindowMemory ( k = 3 , memory_key = \"chat_history\" ) # Initialize the conversational retrieval chain qa = ConversationalRetrievalChain . from_llm ( llm = ChatOpenAI ( model_name = llm_name , temperature = 0.2 , openai_api_key = openai_api_key ), chain_type = \"map_reduce\" , retriever = compression_retriever , memory = memory , get_chat_history = lambda h : h , verbose = True ) return qa","title":"llm service"},{"location":"llm/#services.llm.gpt_3_5_turbo.display_chat_history","text":"Prints the chat history to the console. Each message is displayed with the sender's role and content. Parameters: Name Type Description Default messages list of dict A list of dictionaries where each dictionary represents a message in the chat history. Each message has a 'role' key indicating who sent the message and a 'content' key with the message text. required Source code in app/services/llm/gpt_3_5_turbo.py 12 13 14 15 16 17 18 19 20 21 def display_chat_history ( messages ): \"\"\" Prints the chat history to the console. Each message is displayed with the sender's role and content. Args: messages (list of dict): A list of dictionaries where each dictionary represents a message in the chat history. Each message has a 'role' key indicating who sent the message and a 'content' key with the message text. \"\"\" for message in messages : print ( f \" { message [ 'role' ] . capitalize () } : { message [ 'content' ] } \" )","title":"display_chat_history"},{"location":"llm/#services.llm.gpt_3_5_turbo.get_assistant_response","text":"Sends the current chat history to the OpenAI API to generate a response from the assistant. Parameters: Name Type Description Default messages list of dict The current chat history as a list of message dictionaries. required Returns: Name Type Description str The assistant's response as a string. Raises: Type Description Exception Prints an error message if the API call fails and returns a default error response. Source code in app/services/llm/gpt_3_5_turbo.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def get_assistant_response ( messages ): \"\"\" Sends the current chat history to the OpenAI API to generate a response from the assistant. Args: messages (list of dict): The current chat history as a list of message dictionaries. Returns: str: The assistant's response as a string. Raises: Exception: Prints an error message if the API call fails and returns a default error response. \"\"\" try : r = client . chat . completions . create ( model = \"gpt-4\" , # The model version to use for generating responses, adjust as needed messages = [{ \"role\" : m [ \"role\" ], \"content\" : m [ \"content\" ]} for m in messages ], ) response = r . choices [ 0 ] . message . content return response except Exception as e : print ( f \"An error occurred: { e } \" ) return \"Sorry, I can't process your request right now.\"","title":"get_assistant_response"},{"location":"llm/#services.llm.gpt_3_5_turbo.get_response","text":"Processes a user's prompt to generate and display the assistant's response using the OpenAI GPT model. Parameters: Name Type Description Default prompt str The user's message to which the assistant should respond. required Returns: Name Type Description str The assistant's response, which is also added to the chat history and displayed along with the rest of the conversation. Source code in app/services/llm/gpt_3_5_turbo.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def get_response ( prompt : str ): \"\"\" Processes a user's prompt to generate and display the assistant's response using the OpenAI GPT model. Args: prompt (str): The user's message to which the assistant should respond. Returns: str: The assistant's response, which is also added to the chat history and displayed along with the rest of the conversation. \"\"\" # Add the user's message to the chat history messages . append ({ \"role\" : \"user\" , \"content\" : prompt }) # Get the assistant's response and add it to the chat history response = get_assistant_response ( messages ) messages . append ({ \"role\" : \"assistant\" , \"content\" : response }) # Display the updated chat history display_chat_history ( messages ) return response","title":"get_response"},{"location":"llm/#services.llm.llm_preprocessing.chain_workflow","text":"Initializes and configures a conversational retrieval chain with document understanding and storage capabilities. This function configures a complete workflow for a conversational AI system, including PDF document loading, document splitting, embedding, and compression-based retrieval from a conversational context using LangChain and OpenAI technologies. The setup involves creating a vector database if it doesn't exist, and utilizing it for context retrieval in conversation. Parameters: Name Type Description Default openai_api_key str API key for OpenAI services, used for both embedding generation and conversational responses. required Returns: Name Type Description ConversationalRetrievalChain A configured LangChain conversational retrieval chain, ready for deploying in a chatbot system. Source code in app/services/llm/llm_preprocessing.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 @st . cache_resource def chain_workflow ( openai_api_key ): \"\"\" Initializes and configures a conversational retrieval chain with document understanding and storage capabilities. This function configures a complete workflow for a conversational AI system, including PDF document loading, document splitting, embedding, and compression-based retrieval from a conversational context using LangChain and OpenAI technologies. The setup involves creating a vector database if it doesn't exist, and utilizing it for context retrieval in conversation. Args: openai_api_key (str): API key for OpenAI services, used for both embedding generation and conversational responses. Returns: ConversationalRetrievalChain: A configured LangChain conversational retrieval chain, ready for deploying in a chatbot system. \"\"\" llm_name = \"gpt-3.5-turbo\" # LLM model name for chat interactions # Directory to persist vector index persist_directory = os . environ . get ( 'VECTOR_INDEX_PATH' , 'vector_index/' ) # Load OpenAI embedding model embeddings = OpenAIEmbeddings ( openai_api_key = openai_api_key ) # Check if the vectorstore exists if not os . path . exists ( os . environ . get ( \"DB_PATH\" , \"vector_index/chroma.sqlite3\" )): # Load and split documents if vectorstore does not exist file = os . environ . get ( \"DOCUMENTS_PATH\" , \"documents/DoniBara.pdf\" ) loader = PyPDFLoader ( file ) documents = loader . load () text_splitter = RecursiveCharacterTextSplitter ( chunk_size = 1000 , chunk_overlap = 150 ) splits = text_splitter . split_documents ( documents ) vectordb = Chroma . from_documents ( documents = splits , embedding = embeddings , persist_directory = persist_directory ) vectordb . persist () print ( \"Vectorstore created and saved successfully. The 'chroma.sqlite3' file has been created.\" ) else : # Load existing vectorstore vectordb = Chroma ( persist_directory = persist_directory , embedding_function = embeddings ) # Load OpenAI chat model llm = ChatOpenAI ( temperature = 0.2 , openai_api_key = openai_api_key ) # Setup document compressor and retrieval system compressor = LLMChainExtractor . from_llm ( llm ) compression_retriever = ContextualCompressionRetriever ( base_compressor = compressor , base_retriever = vectordb . as_retriever ( search_type = \"mmr\" , search_kwargs = { \"k\" : 3 }) ) # Setup conversation memory for retaining context memory = ConversationBufferWindowMemory ( k = 3 , memory_key = \"chat_history\" ) # Initialize the conversational retrieval chain qa = ConversationalRetrievalChain . from_llm ( llm = ChatOpenAI ( model_name = llm_name , temperature = 0.2 , openai_api_key = openai_api_key ), chain_type = \"map_reduce\" , retriever = compression_retriever , memory = memory , get_chat_history = lambda h : h , verbose = True ) return qa","title":"chain_workflow"},{"location":"models/","text":"ImageModel Bases: BaseModel A model representing image data for processing and prediction in the API. Attributes: Name Type Description image_name str The name or path where the image is stored. This is used to fetch the image for analysis. sensitive_structures List [ str ] A list of structures or areas in the image that are considered sensitive. This information is used to provide contextual analysis based on the prediction results. incident_id str A unique identifier for the incident depicted in the image. This is used for tracking and storing results in the database. Source code in app/models/image_model.py 4 5 6 7 8 9 10 11 12 13 14 15 16 class ImageModel ( BaseModel ): \"\"\" A model representing image data for processing and prediction in the API. Attributes: image_name (str): The name or path where the image is stored. This is used to fetch the image for analysis. sensitive_structures (List[str]): A list of structures or areas in the image that are considered sensitive. This information is used to provide contextual analysis based on the prediction results. incident_id (str): A unique identifier for the incident depicted in the image. This is used for tracking and storing results in the database. \"\"\" #Id: int # Uncomment if an ID attribute is necessary image_name : str sensitive_structures : List [ str ] incident_id : str","title":"Pydantic model"},{"location":"models/#models.image_model.ImageModel","text":"Bases: BaseModel A model representing image data for processing and prediction in the API. Attributes: Name Type Description image_name str The name or path where the image is stored. This is used to fetch the image for analysis. sensitive_structures List [ str ] A list of structures or areas in the image that are considered sensitive. This information is used to provide contextual analysis based on the prediction results. incident_id str A unique identifier for the incident depicted in the image. This is used for tracking and storing results in the database. Source code in app/models/image_model.py 4 5 6 7 8 9 10 11 12 13 14 15 16 class ImageModel ( BaseModel ): \"\"\" A model representing image data for processing and prediction in the API. Attributes: image_name (str): The name or path where the image is stored. This is used to fetch the image for analysis. sensitive_structures (List[str]): A list of structures or areas in the image that are considered sensitive. This information is used to provide contextual analysis based on the prediction results. incident_id (str): A unique identifier for the incident depicted in the image. This is used for tracking and storing results in the database. \"\"\" #Id: int # Uncomment if an ID attribute is necessary image_name : str sensitive_structures : List [ str ] incident_id : str","title":"ImageModel"},{"location":"system-arch/","text":"","title":"System Arch"}]}